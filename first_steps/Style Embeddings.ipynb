{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on running this notebook:\n",
    "Copy the repository https://github.com/dennybritz/cnn-text-classification-tf one directory up from this one  \n",
    "Switch to a virtual environment with tensorflow==1.9  \n",
    "cd ../cnn-text-classification-tf\n",
    "run ./train.py\n",
    "Using the same environment you used to run the training script, you should be able to run this notebook.  \n",
    "The output is a numpy array with the embedding of each sentence  \n",
    "The inputs to the model are word-tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aegisworn\\Anaconda3\\envs\\style_detection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Aegisworn\\Anaconda3\\envs\\style_detection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Aegisworn\\Anaconda3\\envs\\style_detection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Aegisworn\\Anaconda3\\envs\\style_detection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Aegisworn\\Anaconda3\\envs\\style_detection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Aegisworn\\Anaconda3\\envs\\style_detection\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('../cnn-text-classification-tf/')\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "import csv\n",
    "assert tf.__version__[0] == '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "checkpoint_dir = '../cnn-text-classification-tf/runs/1604974730/checkpoints/'\n",
    "positive_data_file = '../cnn-text-classification-tf/data/rt-polaritydata/rt-polarity.pos'\n",
    "negative_data_file = '../cnn-text-classification-tf/data/rt-polaritydata/rt-polarity.neg'\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aegisworn\\Anaconda3\\envs\\style_detection\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:203: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "x_raw, y_test = data_helpers.load_data_and_labels(positive_data_file, negative_data_file)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "vocab_path = os.path.join(checkpoint_dir, '..', 'vocab')\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal\", \"the gorgeously elaborate continuation of the lord of the rings trilogy is so huge that a column of words cannot adequately describe co writer director peter jackson 's expanded vision of j r r tolkien 's middle earth\", 'effective but too tepid biopic', 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start', \"emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one\", 'the film provides some great insight into the neurotic mindset of all comics even those who have reached the absolute top of the game', 'offers that rare combination of entertainment and education', 'perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions', \"steers turns in a snappy screenplay that curls at the edges it 's so clever you want to hate it but he somehow pulls it off\", 'take care of my cat offers a refreshingly different slice of asian cinema']\n"
     ]
    }
   ],
   "source": [
    "# example input sentences\n",
    "print(x_raw[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Aegisworn\\code\\deep_fakes\\Project\\cnn-text-classification-tf\\runs\\1604974730\\checkpoints\\model-30000\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=allow_soft_placement,\n",
    "      log_device_placement=log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        embeddings = graph.get_operation_by_name('dropout/dropout/mul').outputs[0]\n",
    "        \n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "\n",
    "        for i, x_test_batch in enumerate(batches):\n",
    "            batch_embeddings = sess.run(embeddings, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            if i == 0:\n",
    "                all_embeddings = batch_embeddings\n",
    "            else:\n",
    "                all_embeddings = np.concatenate([all_embeddings, batch_embeddings], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10662, 384)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style_detection",
   "language": "python",
   "name": "style_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
